---

title: spark源码调试环境
date: 2017-07-12

---


# Apache Spark 源码阅读环境搭建

最近工作需要使用 [Apache Spark](http://spark.apache.org/) ，于是打算顺便学习一下源码，这里记录一下搭建环境的过程。

## 前置条件

* sbt
* maven
* java
* scala
* git
* IntelliJ IDEA


- __IntelliJ IDEA 安装 Scala 插件__

依次选择 Preferences -> Plugins -> Install JetBrains plugin。搜索框输入 scala，右侧点击 Install Plugin，安装成功后重启 IntelliJ IDEA。



- __IntelliJ IDEA 导入 Apache Spark {{book.sparkVersion}} 源码__

使用 IntelliJ IDEA 打开 Apache Spark {{book.sparkVersion}} 源码目录下的 pom.xml 文件。

导入项目完成后，依次打开 Project Structure -> Modules -> spark-streaming-flume-sink.2.11，右键 target 目录，取消 Excluded 标签。

展开到 target -> scala-2.11 -> src_managed -> main -> compiled_avro 目录，右键，标记为 Source 目录。

同样，将 spark-hive\_2.10 模块内的 v0.13.1 -> src -> main -> scala 目录标记为 Source 目录。

点击 Build -> Rebuild Project，等待项目构建完成即可。

## 坑

### Spark SQL has no SparkSqlParser.scala file when compiling in intelliJ idea

解决方案： 
  下载antlr插件 
  打开 View->Tool Windows->Maven Projects， 选择"Spark Project Catalyst"并右击，然后选"Generate sources and update folders"

### Exception in thread "main" java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce$class

解决方案：
    Project Structures -> Modules -> spark-examples Dependencies 标签下添加jar包, {spark dir}/spark/assembly/target/scala-2.11/jars/

